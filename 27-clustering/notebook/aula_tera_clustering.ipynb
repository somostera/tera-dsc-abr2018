{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TERA - Aula 27\n",
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objetivos gerais de algoritmos de clustering:\n",
    "- Análise exploratória dos dados\n",
    "- Encontrar padrões e estruturas\n",
    "- Agrupar dados de forma a criar representações sumarizadas (sumarização de dados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Índice\n",
    "\n",
    "- [Exemplo inicial](#Exemplo-Inicial)\n",
    "- [K-Means](#K-Means)\n",
    " - [Case K-Means Elo7](#Case-Cluster-Usuários-Elo7)\n",
    "- [Case Elo7 - Cluster Frete](#Case-Elo7---Clustering-de-Frete)\n",
    "- [Hierarchical Clustering](#Hierarchical-Clustering)\n",
    " - [Exercício Prático](#Exercício-prático-Hierarchical-Clustering)\n",
    "- [Comparação Métodos Clustering](#Comparação-métodos-clustering:)\n",
    "- [Case Elo7 - Motivos de Compra](#Case-Elo7---Motivos-de-Compra)\n",
    "- [Extra - DBSCAN](#Extra---DBSCAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplo Inicial\n",
    "Análise exploratória do comportamento dos usuários do Elo7.\n",
    "\n",
    "Dataset:\n",
    "- `tempo` (float): Tempo em segundos que um usuário permanece no site.\n",
    "- `ticket` (float): Valor gasto em reais no site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports usados no curso\n",
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as ss\n",
    "\n",
    "sns.set(style=\"ticks\")\n",
    "plt.rcParams['figure.figsize'] = (12.0, 8.0)\n",
    "plt.style.use('seaborn-colorblind')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pasta contendo os dados:\n",
    "ROOT_FOLDER = os.path.realpath('..')\n",
    "DATASET_FOLDER = os.path.join(ROOT_FOLDER,'datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Leitura dos dados\n",
    "df_user_elo7 = pd.read_csv(os.path.join(DATASET_FOLDER, 'user_patterns_elo7_dataset.csv'), sep=';')\n",
    "\n",
    "df_user_elo7.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consegue fazer uma análise exploratória nos dados? Existe alguma correlação entre o tempo de permanência no site e o valor gasto no marketplace?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A análise fez sentido? Será que podemos analisar os dados como um todo ou existem segmentos com comportamentos diferentes? *Dica: procure por [K-Means](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemplo Clustering (opcional)\n",
    "\n",
    "[Pular](#Case-Cluster-Usuários-Elo7)\n",
    "\n",
    "Vamos para um problema clássico.\n",
    "Utilizaremos o dataset Iris. O dataset contém os seguintes atributos:\n",
    "\n",
    "- `sepal_length`: Comprimento da sépala da flor\n",
    "- `sepal_width`: Largura da sépala\n",
    "- `petal_length`: Comprimento da pétala\n",
    "- `petal_width`: Largura da pétala\n",
    "- `species`: Espécie da flor\n",
    "\n",
    "![iris](https://cdn-images-1.medium.com/max/1600/1*1q79O5DCx_XNrAARXSFzpg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importe o dataset\n",
    "df_iris = pd.read_csv(os.path.join(DATASET_FOLDER,'iris_dataset.csv'))\n",
    "\n",
    "df_iris.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como temos um vetor de features de 4 dimensões, não faz sentido tentarmos visualizar todas as dimensões em uma só figura. Entretanto, podemos visualizar a relação entre cada uma de suas features através de um mapa de dispersão pareado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Scatter plot pareado utilizando o Seaborn\n",
    "sns.pairplot(df_iris, hue=\"species\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos perceber que há uma boa separação de clusters utilizando as features `petal_length` e `petal_width`. Podemos utilizar apenas essas dimensões para a nossa análise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Separe o dataset utilizando apenas as features `petal_length` e `petal_width` do dataframe\n",
    "# Coloque essas colunas em um numpy array (dica: utilize o atributo df[...].values)\n",
    "X = df_iris[['petal_length','petal_width']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot dos dados\n",
    "plt.scatter(x=X[:,0],y=X[:,1])\n",
    "plt.xlabel('petal_length')\n",
    "plt.ylabel('petal_width')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nós já sabemos que existem três espécies no dataset, mas podemos tentar analisar se conseguiríamos encontrar esses clusters de maneira automática. Vamos utilizar o famoso algoritmo [**K-Means**](https://en.wikipedia.org/wiki/K-means_clustering) para achar esses clusters. Verifique no site do [scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) mais detalhes de implementação do algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importe o módulo do KMeans \n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Crie uma instância do K-Means pelo sklearn\n",
    "kmeans = KMeans(n_clusters=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Treine e aplique o modelo KMeans para o dataset X\n",
    "results = kmeans.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Crie uma coluna no dataframe df para incluir os resultados\n",
    "df_iris['cluster'] = results\n",
    "\n",
    "df_iris.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plote o mapa de dispersão (scatter plot) dos clusters gerados\n",
    "# Veja o exemplo das moedas para se inspirar\n",
    "sns.swarmplot(data=df_iris, x='petal_length', y='petal_width', hue='cluster')\n",
    "plt.xlabel('petal_length')\n",
    "plt.ylabel('petal_width')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os clusters formados fazem sentido? Veja novamente o gráfico da separação das espécies para ver se os clusters têm relação com elas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.swarmplot(data=df_iris, x='petal_length', y='petal_width', hue=\"species\")\n",
    "plt.xlabel('petal_length')\n",
    "plt.ylabel('petal_width')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos também utilizar a matriz de tabulação cruzada para verificar a relação dos clusters com as espécies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Apresente a matriz de cross-tabulation (Veja o exemplo das moedas)\n",
    "ct = pd.crosstab(df_iris['cluster'], df_iris['species'])\n",
    "print(ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quais são as conclusões? O K-Means funcionou do jeito que era esperado?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Escolha do número de clusters\n",
    "\n",
    "Nós temos diversos métodos para escolher o número ideal de clusters. Alguns deles estão resumidos neste [artigo](https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set#The_Elbow_Method). O método mais utilizado, entretanto, é o método do \"cotovelo\" (*elbow method*). \n",
    "\n",
    "Mas, antes de falarmos do método do cotovelo, nós precisamos definir o que é um bom cluster. É claro que isso depende de cada caso, mas as seguintes características são desejadas para a maioria dos clusters:\n",
    "- Dados não muito dispersos -> Inércia\n",
    "- Dados dentro dos clusters possuem perfil semelhante\n",
    "- Quantidade aproximadamente uniforme de dados em cada cluster (controverso)\n",
    "\n",
    "#### Inércia\n",
    "\n",
    "A inércia de um cluster é definida como a soma das distâncias quadráticas de cada ponto de um cluster ao seu respectivo centroide, somada através de todos os clusters. Quanto maior é a inércia, maior será a dispersão dos clusters. Portanto, desejamos escolher um número de clusters que nos possibilite ter uma inércia baixa. Simples, mas temos um problema... O mínimo valor de inércia que podemos obter é quando cada ponto do nosso dataset pertence ao seu próprio cluster. Portanto, precisamos escolher um balanço entre baixa inércia e baixo número de clusters. \n",
    "\n",
    "Para isso, utilizamos o gráfico de cotovelo. O eixo horizontal do gráfico representa o número de clusters utilizados e o eixo vertical representa a inércia total dos clusters. O número de clusters ideal é definido como o ponto onde o gráfico se aproxima a uma horizontal (como o ponto de encontro do braço e antebraço)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos apresentar o gráfico de inércia do problema anterior e verificar se escolhemos corretamente o número de clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Range de valores de clusters que vamos testar\n",
    "k = range(1,8,1)\n",
    "\n",
    "# Lista de inércias\n",
    "inertias = []\n",
    "\n",
    "# Para cada valor de k, ache a inércia\n",
    "for i in k:\n",
    "    # crie a instância\n",
    "    kmeans = KMeans(n_clusters=i)\n",
    "\n",
    "    # Treine o modelo\n",
    "    model = kmeans.fit(X)\n",
    "\n",
    "    # Ache a inercia dos clusters\n",
    "    inertias.append(model.inertia_)\n",
    "    \n",
    "plt.plot(k, inertias, '-ob')\n",
    "plt.xlabel('Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos verificar que o número de clusters ideal é 3. Essa análise nos mostra que escolhemos corretamente o número de clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Case Cluster Usuários Elo7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos encontrar o número ideal de clusters da distribuição de usuários do Elo7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Veja novamente os dados\n",
    "df_user_elo7.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vamos plotar o gráfico\n",
    "df_user_elo7.plot.scatter(x='tempo',y='ticket', alpha=0.5)\n",
    "plt.xlabel('Tempo (s)')\n",
    "plt.ylabel('Ticket (R$)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore o método K-Means para encontrar os clusters. Quantos clusters devemos escolher?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = df_user_elo7.values\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos encontrar o número ideal de clusters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qual a sua opinião? Quantos clusters devemos utilizar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Case Elo7 - Clustering de Frete\n",
    "\n",
    "Um dos problemas mais complicados do Elo7 é sua dependência dos correios. Nós sofremos muito com a falta de alternativas para dar aos nossos clientes (compradores e vendedores), já que o serviço dos correios além de caro, é também instável. \n",
    "\n",
    "Para tentar resolver esse problema, o time de Data Science do Elo7 foi chamado para tentar encontrar alguma alternativa. Após algumas conversas, nós levantamos a possibilidade de utilizarmos serviços de entrega independentes dos correios. Mas, o problema é que esses serviços necessitam de um volume grande de encomendas por ponto de coleta, o que não é o caso para a maioria dos vendedores cadastrados no Elo7. \n",
    "\n",
    "Uma possível solução seria encontrar pontos de coleta que pudessem agregar pedidos de vários vendedores e enviar de uma vez só com um desses serviços alternativos. Mas, como obtemos a localização desses pontos de coleta? Podemos aplicar um algoritmo de clustering nas rotas de frete mais frequentes!\n",
    "\n",
    "Vamos tentar analisar os dados e verificar o que conseguimos obter. O dataset a seguir contém pares de endereços de origem e destino de entregas realizadas apenas na cidade de São Paulo em um curto intervalo de tempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_route = pd.read_csv(os.path.join(DATASET_FOLDER, 'route_clustering_elo7_dataset.csv'), sep=';')\n",
    "\n",
    "df_route.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para facilitar os cálculos de distância, as latitudes e longitudes dos locais já foram realizados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora formar nosso vetor de features contendo as posições geográficas das nossas rotas.\n",
    "\n",
    "*Dica: Será que é necessário normalizar as features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "X = df_route[['latitude_origem','longitude_origem','latitude_destino','longitude_destino']].values\n",
    "X_scaled = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantos clusters vamos utilizar? (Obs: Podemos aplicar o método do cotovelo para descobrir.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Range de valores de clusters que vamos testar\n",
    "k = range(1,10,1)\n",
    "\n",
    "# Lista de inércias\n",
    "inertias = []\n",
    "\n",
    "# Para cada valor de k, ache a inércia\n",
    "for i in k:\n",
    "    # crie a instância\n",
    "    kmeans = KMeans(n_clusters=i)\n",
    "\n",
    "    # Treine o modelo\n",
    "    model = kmeans.fit(X_scaled)\n",
    "\n",
    "    # Ache a inercia dos clusters\n",
    "    inertias.append(model.inertia_)\n",
    "    \n",
    "plt.plot(k, inertias, '-ob')\n",
    "plt.xlabel('Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora podemos iniciar o algoritmo de clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "kmeans = KMeans(n_clusters=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "labels = kmeans.fit_predict(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A análise da quantidade de ítens em cada cluster é sempre uma boa prática. Clusters desbalanceados são um sinal de que os dados não foram bem separados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label, count = np.unique(labels, return_counts=True)\n",
    "for l, c in zip(label,count):\n",
    "    print('Cluster {}: {}'.format(l,c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos ver os gráficos para analisar qualitativamente os resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = kmeans.labels_\n",
    "\n",
    "ax1 = plt.subplot(1,2,1)\n",
    "ax1.set_title('Origem')\n",
    "plt.scatter(x=X[:,0],\n",
    "            y=X[:,1],\n",
    "            c=labels, \n",
    "            edgecolor='k',\n",
    "            cmap='rainbow')\n",
    "ax1.set_xlim((-23.4,-23.9))\n",
    "\n",
    "ax2 = plt.subplot(1,2,2)\n",
    "ax2.set_title('Destino')\n",
    "plt.scatter(x=X[:,2],\n",
    "            y=X[:,3],\n",
    "            c=labels, \n",
    "            edgecolor='k',\n",
    "            cmap='rainbow')\n",
    "ax2.set_xlim((-23.4,-23.9))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O que achou? É possível perceber clusters bem definidos? Será que podemos utilizar esses clusters para resolver nossos problemas de frete?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Exemplo Prático - Kaggle NYC Taxi Trip Duration\n",
    "\n",
    "Dados:\n",
    "- id - a unique identifier for each trip\n",
    "- vendor_id - a code indicating the provider associated with the trip record\n",
    "- pickup_datetime - date and time when the meter was engaged\n",
    "- dropoff_datetime - date and time when the meter was disengaged\n",
    "- passenger_count - the number of passengers in the vehicle (driver entered value)\n",
    "- pickup_longitude - the longitude where the meter was engaged\n",
    "- pickup_latitude - the latitude where the meter was engaged\n",
    "- dropoff_longitude - the longitude where the meter was disengaged\n",
    "- dropoff_latitude - the latitude where the meter was disengaged\n",
    "- store_and_fwd_flag - This flag indicates whether the trip record was held in vehicle memory before sending to the vendor because the vehicle did not have a connection to the server - Y=store and forward; N=not a store and forward trip\n",
    "- trip_duration - duration of the trip in seconds\n",
    "\n",
    "![iris](https://cdn.civitatis.com/estados-unidos/nueva-york/galeria/thumbs/taxi-nueva-york.jpg)\n",
    "\n",
    "*Solução retirada do github de [juifa-tsai](https://github.com/juifa-tsai/NYC_Taxi_Trip_Duration)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos realizar uma análise dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_taxi = pd.read_csv(os.path.join(DATASET_FOLDER,'nyc_trip_duration_dataset.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_taxi.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos utilizar diversas abordagens para analisar os dados. Vamos tentar verificar os dados de localização dos passageiros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_map = df_taxi[['pickup_longitude','pickup_latitude', 'dropoff_longitude','dropoff_latitude']]\n",
    "df_pick = df_map[['pickup_longitude','pickup_latitude']]\n",
    "df_drop = df_map[['dropoff_longitude','dropoff_latitude']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos visualizar os dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot_map(df, zoom=0.9):\n",
    "    cutmap = zoom/100\n",
    "\n",
    "    x = df['pickup_longitude']\n",
    "    y = df['pickup_latitude']\n",
    "    x_max, x_min = x.quantile(1-cutmap), x.quantile(cutmap)\n",
    "    y_max, y_min = y.quantile(1-cutmap), y.quantile(cutmap)\n",
    "    \n",
    "    x_plot = x[(x>x_min) & (x<x_max) & (y<y_max) & (y>y_min)]\n",
    "    y_plot = y[(x>x_min) & (x<x_max) & (y<y_max) & (y>y_min)]\n",
    "    plt.scatter(x=x_plot, y=y_plot, s=5, alpha=0.3)\n",
    "    plt.tick_params(labelsize=18)\n",
    "    plt.title('Pickup', fontsize=18 )\n",
    "    plt.xlabel('Longitude', fontsize=18)\n",
    "    plt.ylabel('Latitude',  fontsize=18)\n",
    "    plt.show()\n",
    "\n",
    "plot_map(df_taxi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A distribuição dos dados é bem interessante. Podemos verificar que existe uma concentração grande de pontos dentro da ilha de Manhattan, o que é esperado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como segundo passo da análise dos dados, nós podemos tentar enriquecê-los utilizando técnicas de feature engineering e clustering. Vamos explorar o segundo em seguida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O racional de utilizar clustering para análise exploratória e feature engineering é o fato de encontrar estruturas implícitas nos dados. Por exemplo, se tentássemos observar cada passageiro individualmente, talvez teríamos dificuldade em encontrar um padrão nos dados. Mas, é intuitivo pensar que passageiros semelhantes (mesma localização, horário etc) possam ser agrupados e tratados como um só. Assim, podemos tratar os dados por grupos controlados de passageiros, ao invés de cada indivíduo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos tentar encontrar clusters nos dados de início da corrida de taxi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_kmeans = kmeans.fit_predict(df_pick)\n",
    "df_pick['zone']  = X_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def draw_map_zone( df, x_name, y_name, z_name, name, zoom=0.9, cluster=None ):\n",
    "\n",
    "    x = df[x_name]\n",
    "    y = df[y_name]\n",
    "    z = df[z_name]\n",
    "\n",
    "    cutmap = zoom/100\n",
    "    x_max, x_min = x.quantile(1-cutmap), x.quantile(cutmap)\n",
    "    y_max, y_min = y.quantile(1-cutmap), y.quantile(cutmap)\n",
    "    \n",
    "    zones = np.unique(z[(x>x_min) & (x<x_max) & (y<y_max) & (y>y_min)])\n",
    "\n",
    "    #cmap = plt.get_cmap('spectral') \n",
    "    cmap = plt.get_cmap('winter') \n",
    "    colors = [cmap(i) for i in np.linspace(0, 1, len(zones))]\n",
    "\n",
    "    for i, zone in enumerate(zones):       \n",
    "        plt.scatter( x=x[ (z==zone) & (x>x_min) & (x<x_max) & (y<y_max) & (y>y_min) ], \n",
    "                     y=y[ (z==zone) & (x>x_min) & (x<x_max) & (y<y_max) & (y>y_min) ], \n",
    "                     s=5, alpha=0.3, c=colors[i])\n",
    "        if cluster:\n",
    "            plt.text( cluster.cluster_centers_[zone,0], cluster.cluster_centers_[zone,1], str(zone), fontsize = 12, color='r')\n",
    "\n",
    "    plt.tick_params(labelsize=18)\n",
    "    plt.title(name, fontsize=18 )\n",
    "    plt.xlabel('Longitude', fontsize=18)\n",
    "    plt.ylabel('Latitude',  fontsize=18)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(17,15))\n",
    "draw_map_zone(df_pick, 'pickup_longitude', 'pickup_latitude', 'zone', 'Pickup', cluster=kmeans)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não por acaso, os clusters encontrados se assemelham aos bairros de Nova Iorque. Esses clusters agora podem ser utilizados de diversas formas:\n",
    "- Podemos explorar a distribuição das outras features dentro de cada um dos clusters. Assim poderemos ver o quanto cada região se diferença das outras.\n",
    "- Podemos também utilizar agora as labels obtidas pelo algoritmo de clustering como entrada de outros algoritmos de machine learning. Essa técnica é muito utilizada para melhorar a precisão dos algoritmos de regressão e classificação."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos tentar utilizar clustering na solução encontrada na [aula 15](https://github.com/somostera/tera-dsc-abr2018/blob/master/15-ml-with-decision-trees/notebooks/Gabarito%20Aula%2015%20-%20AM%20com%20%C3%81rvores%20de%20Decis%C3%A3o.ipynb) para verificar se conseguimos aumentar a precisão do regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=5,random_state=0)\n",
    "\n",
    "df_taxi_cluster = df_taxi.copy()\n",
    "\n",
    "kmeans = KMeans(n_clusters=100)\n",
    "X_kmeans = kmeans.fit_predict(df_map)\n",
    "df_taxi_cluster['zone'] = X_kmeans\n",
    "\n",
    "x = df_taxi.drop(['trip_duration', 'id', 'pickup_datetime', 'dropoff_datetime', 'store_and_fwd_flag'], axis=1)\n",
    "y = df_taxi['trip_duration']\n",
    "\n",
    "x_cluster = df_taxi_cluster.drop(['trip_duration', 'id', 'pickup_datetime', 'dropoff_datetime', 'store_and_fwd_flag'], axis=1)\n",
    "\n",
    "reg = DecisionTreeRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_cv_prediction(x,y,train_index,test_index,reg):\n",
    "    x_train, x_test = x.iloc[train_index], x.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    reg.fit(x_train, y_train)\n",
    "    y_pred = reg.predict(x_test)\n",
    "    return y_pred, y_test\n",
    "\n",
    "def rmsle(y_test, y_pred):\n",
    "    return np.sqrt(mean_squared_log_error(y_test,y_pred))\n",
    "\n",
    "rmsle_cv_default = []\n",
    "rmsle_cv_cluster = []\n",
    "for train_index, test_index in kfold.split(x,y):\n",
    "    y_pred, y_test = make_cv_prediction(x,y,train_index,test_index,reg)\n",
    "    rmsle_cv_default.append(rmsle(y_test, y_pred))\n",
    "    \n",
    "    y_pred_cluster, y_test = make_cv_prediction(x_cluster,y,train_index,test_index,reg)\n",
    "    rmsle_cv_cluster.append(rmsle(y_test, y_pred_cluster))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Default: {:.4f}'.format(np.mean(rmsle_cv_default)))\n",
    "print('Cluster: {:.4f}'.format(np.mean(rmsle_cv_cluster)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pode-se notar um ligeiro aumento de precisão do estimador ao utilizar o cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "## Hierarchical Clustering\n",
    "\n",
    "Vamos agora aprender sobre outro método de clustering: [**Hierarchical Clustering**](https://en.wikipedia.org/wiki/Hierarchical_clustering). Como o nome mesmo diz, ele utiliza o conceito de *hierarquia* para construir os clusters. Existem duas principais variações do algoritmo: aglomerativo e por divisão. O primeiro é mais usado na prática. O passo a passo do algoritmo é apresentado abaixo:\n",
    "\n",
    "- Primeiro colocamos todos as observações em clusters próprios;\n",
    "- Depois, iterativamente procuramos os clusters mais próximos\\* e agrupamos eles em um novo cluster;\n",
    "- Repetimos o passo anterior até formarmos um único cluster com todas as observações.\n",
    "\n",
    "\\*Obs: A definição de distância (ou similaridade) entre clusters depende do tipo de métrica de distância (Euclidiana, Manhattan, cosseno etc) e ligação (Ward, simples, completa etc).\n",
    "\n",
    "Como podemos ver no algoritmo, o objetivo é a criação de um grande cluster que agrupe todos os dados. Nós podemos visualizar esse histórico de agrupamentos a partir de um [dendrograma](https://en.wikipedia.org/wiki/Dendrogram). A então criação de clusters mais granulares depende da região de similaridade que se deseja realizar o corte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Exercício Hierarchical Clustering\n",
    "\n",
    "Vamos praticar agora!\n",
    "Utilizaremos o dataset do [Eurovision de 2016](https://eurovision.tv/history/full-split-results) para essa tarefa. Esse evento é uma competição de músicas entre países. Cada país participante seleciona uma música para concorrer com as outras. Ao final, cada país deve votar nas músicas com pontuações entre [1,2,3,4,5,6,7,8,10,12]. Um país pode votar tanto através de um juri formado oficialmente pelo país ou via votos por telefone. Ao final, ganha o país que receber a maior quantidade de pontos.\n",
    "\n",
    "Essa competição é famosa por apresentar um comportamento indesejado: os países próximos geografica e culturalmente tendem a se favorecer. Por essa razão há sempre mudanças nas regras para tentar evitar que isso aconteça. Será que conseguiremos perceber esse comportamento através de um algoritmo de clustering? Vamos tentar!\n",
    "\n",
    "Um país não pode votar em si próprio, mas, para podermos fazer a nossa análise, considerei que um país daria a pontuação máxima (12) para ele próprio.\n",
    "\n",
    "As colunas do dataset são descritas a seguir e consideramos apenas votos feitos por telefone:\n",
    "\n",
    "- `From country`: País votante\n",
    "- `Televote Points`: Pontuação dada por telefone pelo país votante\n",
    "- `To country`: País que recebeu a pontuação do país votante\n",
    "\n",
    "![Eurovision](https://www.eurovisionary.com/wp-content/uploads/2015/11/eurovision-2016.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vamos iniciar a leitura do dataset\n",
    "df_euro = pd.read_csv(os.path.join(DATASET_FOLDER, 'eurovision_dataset.csv'), sep=';')\n",
    "\n",
    "df_euro.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Podemos visualizar quem está ganhando nessa votação\n",
    "df_euro_points = df_euro.groupby(['To country']).sum()\n",
    "df_euro_points.sort_values(by='Televote Points', inplace=True, ascending=False)\n",
    "df_euro_points.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se formos considerar apenas os votos por telefone, a Russia está ganhando a competição!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como fizemos para o exercício anterior, nós temos que tomar cuidado com a distribuição dos dados. como temos valores discretos e determinísticos de pontuação, não faz sentido analisarmos a variância dos dados. Entretanto, podemos normalizar os pontos em uma escala de 0-100%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Obtenha o vetor de pontos\n",
    "X = np.array([i for i in df_euro.groupby('From country')['Televote Points'].apply(np.array)])\n",
    "\n",
    "# TODO\n",
    "# Realize a normalização dos dados\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "normalizer = MaxAbsScaler()\n",
    "\n",
    "X_norm = normalizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos realizar agora o algoritmo Hierarchical Clustering!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importe os métodos linkage (Hierarchical Clustering) e dendrogram\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Aplique o algoritmo Hierarchical Clustering utilizando o scipy\n",
    "# Selecione uma métrica de distância e um método de ligação\n",
    "# Teste vários para obter a intuição por trás de cada método\n",
    "Y = linkage(X_norm, method='ward', metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Crie o dendrograma para visualizar os resultados\n",
    "# Será necessário obter os valores de labels que são os países\n",
    "# *votantes* da competição (Dica: Procure pelo método unique do pandas)\n",
    "plt.figure(figsize=(16,10))\n",
    "labels = pd.unique(df_euro['From country'])\n",
    "dendrogram(Y, \n",
    "           labels=labels, \n",
    "           leaf_rotation=90, \n",
    "           leaf_font_size=14) # Use o leaf_font_size = 14\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O que podemos dizer sobre o dendrograma? Há alguma relação cultural, política ou geográfica entre os países? Se quiséssemos escolher uma região de corte para formar clusters intermediários, qual seria?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemplo Grãos (Opcional)\n",
    "\n",
    "[Pular](#Comparação-métodos-clustering:)\n",
    "\n",
    "Vamos utilizar o dataset de diferentes tipos de grãos de trigo obtidos pelo [UCI](https://archive.ics.uci.edu/ml/datasets/seeds#) para treinar esses conceitos.\n",
    "\n",
    "O dataset contém os seguintes parâmetros:\n",
    "\n",
    "- `area`: Área total do grão, A\n",
    "- `perimeter`: Perímetro do grão, P\n",
    "- `compactness`: Grão de compactação do grão - $C = \\frac{4 \\pi A}{P^2}$\n",
    "- `length_kernel`: Comprimento do núcleo\n",
    "- `width_kernel`: Largura do núcleo\n",
    "- `asymmetry`: Coeficiente de assimetria\n",
    "- `kernel_groove`: Comprimento do sulco do núcleo\n",
    "\n",
    "Variedades de grãos: 'Kama' (1), 'Rosa' (2) e 'Canadian' (3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importar os dados\n",
    "df_grain = pd.read_csv(os.path.join(DATASET_FOLDER, 'seeds_dataset.csv'), sep=';')\n",
    "df_grain.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de iniciarmos o processo de clustering, vamos verificar se há muita discrepância entre as variâncias das features. Essa etapa é muito importante, porque features com variância elevada possuem maior influência na medida de distância do algoritmo do que features com menor variância, o que pode ser indesejado. Quando normalizamos as features, nós conseguimos dar influências iguais para todas elas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_grain.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percebemos que o atributo `area` possui maior variância, enquanto o `compactness` possui baixa variância. Portanto, vamos primeiramente normalizar as features utilizando o [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) do scikit-learn. Ele normaliza as features individualmente deixando a variância delas igual a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vamos importar o StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vamos criar agora o vetor de atributos\n",
    "X = df_grain[['area','perimeter','compactness','length_kernel','width_kernel','asymmetry','kernel_groove']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Agora precisamos normalizar o vetor de features\n",
    "\n",
    "# Primeiro criamos uma instância do StandardScaler\n",
    "normalizer = StandardScaler()\n",
    "\n",
    "# Agora podemos normalizar através do método fit_transform\n",
    "X_norm = normalizer.fit_transform(X)\n",
    "\n",
    "# Podemos verificar que a variância de cada feature é 1\n",
    "np.var(X_norm, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O scikit-learn possui um método próprio para o algoritmo de [Hierarchical Clustering](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering). Entretanto, ele não nos permite visualizar facilmente o dendrograma final. Por isso, vamos utilizar a versão do scipy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importe os métodos linkage (Hierarchical Clustering) e dendrogram\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vamos escolher a métrica de distância:\n",
    "distance = 'euclidean'\n",
    "# Agora o tipo de ligação\n",
    "linkage_type = 'complete'\n",
    "\n",
    "# Vamos aplicar o método linkage\n",
    "Y = linkage(X_norm, method=linkage_type, metric=distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Agora estamos prontos para plotar o dendrograma\n",
    "# Vamos obter o nome das variedades dos grãos\n",
    "varieties = df_grain['varieties'].values\n",
    "\n",
    "# Construímos finalmente o dendrograma\n",
    "plt.figure(figsize=(16,10))\n",
    "dendrogram(Y,\n",
    "           labels=varieties,\n",
    "           leaf_rotation=90,\n",
    "           leaf_font_size=6,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos notar que os clusters formados a partir do Hierarchical Clustering fazem bastante sentido com relação às variedades dos grãos de trigo. Vamos realizar agora um corte no dendrograma de forma a ficarmos com apenas 3 clusters, que representa uma distância de 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Para essa tarefa nós podemos usar o método [`fcluster`](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.cluster.hierarchy.fcluster.html) do scipy. Ele nos permite realizar um corte na árvore de clustering gerada pelo Hierarchical Clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import fcluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vamos gerar os rótulos para os clustes\n",
    "num_clusters = 3\n",
    "labels = fcluster(Y, num_clusters ,criterion='maxclust')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vamos agora criar um dataframe para podermos utilizar o cross-tabulation do pandas\n",
    "df = pd.DataFrame({'labels': labels, 'varieties': varieties})\n",
    "\n",
    "# Crie a matriz de tabulação cruzada\n",
    "ct = pd.crosstab(df['labels'], df['varieties'])\n",
    "\n",
    "print(ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos notar que o as variedades Canadian e Rosa foram muito bem agrupados nos clusters, mas a Kama não teve o mesmo sucesso. Vamos tentar utilizar o método \"Ward\" para a ligação e ver se há alguma variação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Distância\n",
    "distance = 'euclidean'\n",
    "# Ligação\n",
    "linkage_type = 'ward'\n",
    "\n",
    "# Treinar modelo Hierarchical Clustering\n",
    "Y = linkage(X_norm, method=linkage_type, metric=distance)\n",
    "\n",
    "# Plota dendrograma\n",
    "plt.figure(figsize=(16,10))\n",
    "dendrogram(Y,\n",
    "           labels=varieties,\n",
    "           leaf_rotation=90,\n",
    "           leaf_font_size=6,\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Número de clusters\n",
    "num_clusters = 3\n",
    "\n",
    "# Obtem labels\n",
    "labels = fcluster(Y, num_clusters ,criterion='maxclust')\n",
    "\n",
    "# Vamos agora criar um dataframe para podermos utilizar o cross-tabulation do pandas\n",
    "df = pd.DataFrame({'labels': labels, 'varieties': varieties})\n",
    "\n",
    "# Crie a matriz de tabulação cruzada\n",
    "ct = pd.crosstab(df['labels'], df['varieties'])\n",
    "\n",
    "print(ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora temos um resultado melhor para a variedade Kama, mas piorou um pouco o resultado para a Canadian. Dificilmente teremos um algoritmo que consegue ser perfeito em todos os casos. Mas, como podemos visualizar no dendrograma e na matriz de tabulação cruzada, nós conseguimos um bom resultado de clusterização."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "## Comparação métodos clustering:\n",
    "### K-Means x Hierarchical Clustering x DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos testar os 3 algoritmos utilizando alguns datasets padrão do scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import cycle, islice\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# ============\n",
    "# Gera os dados\n",
    "# ============\n",
    "n_samples = 1500\n",
    "# Circulos concentricos\n",
    "noisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5,\n",
    "                                      noise=.05)\n",
    "# Formato de lua\n",
    "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05)\n",
    "# Bolas\n",
    "blobs = datasets.make_blobs(n_samples=n_samples, random_state=8)\n",
    "# Sem estrutura (uniforme)\n",
    "no_structure = np.random.rand(n_samples, 2), None\n",
    "\n",
    "# Dados distribuídos anisotropicamente\n",
    "random_state = 170\n",
    "X, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\n",
    "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
    "X_aniso = np.dot(X, transformation)\n",
    "aniso = (X_aniso, y)\n",
    "\n",
    "# blobs with varied variances\n",
    "varied = datasets.make_blobs(n_samples=n_samples,\n",
    "                             cluster_std=[1.0, 2.5, 0.5],\n",
    "                             random_state=random_state)\n",
    "\n",
    "# ============\n",
    "# Ajusta os parametros dos modelos e datasets\n",
    "# ============\n",
    "plt.figure(figsize=(16, 20))\n",
    "plt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05,\n",
    "                    hspace=.01)\n",
    "\n",
    "plot_num = 1\n",
    "\n",
    "# Parametros dos modelos de clustering\n",
    "default_base = {'quantile': .3,\n",
    "                'eps': .3,\n",
    "                'damping': .9,\n",
    "                'preference': -200,\n",
    "                'n_neighbors': 10,\n",
    "                'n_clusters': 3}\n",
    "\n",
    "# Parametros do dataset\n",
    "datasets = [\n",
    "    (noisy_circles, {'damping': .77, 'preference': -240,\n",
    "                     'quantile': .2, 'n_clusters': 2}),\n",
    "    (noisy_moons, {'damping': .75, 'preference': -220, 'n_clusters': 2}),\n",
    "    (varied, {'eps': .18, 'n_neighbors': 2}),\n",
    "    (aniso, {'eps': .15, 'n_neighbors': 2}),\n",
    "    (blobs, {}),\n",
    "    (no_structure, {})]\n",
    "\n",
    "for i_dataset, (dataset, algo_params) in enumerate(datasets):\n",
    "    # Atualiza os parametros para o dataset especifico\n",
    "    params = default_base.copy()\n",
    "    params.update(algo_params)\n",
    "\n",
    "    X, y = dataset\n",
    "\n",
    "    # Normaliza o dataset\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # ============\n",
    "    # Criação dos objetos de clustering\n",
    "    # ============\n",
    "    \n",
    "    # K-Means\n",
    "    two_means = cluster.KMeans(n_clusters=params['n_clusters'])\n",
    "    \n",
    "    # DBSCAN\n",
    "    dbscan = cluster.DBSCAN(eps=params['eps'])\n",
    "    \n",
    "    # Hierarchical Clustering - Aglomerativo\n",
    "    ward = cluster.AgglomerativeClustering(\n",
    "        linkage=\"ward\",\n",
    "        n_clusters=params['n_clusters'])\n",
    "\n",
    "    clustering_algorithms = (\n",
    "        ('KMeans', two_means),\n",
    "        ('HierarchicalClustering', ward),\n",
    "        ('DBSCAN', dbscan),\n",
    "    )\n",
    "\n",
    "    for name, algorithm in clustering_algorithms:\n",
    "        algorithm.fit(X)\n",
    "\n",
    "        if hasattr(algorithm, 'labels_'):\n",
    "            y_pred = algorithm.labels_.astype(np.int)\n",
    "        else:\n",
    "            y_pred = algorithm.predict(X)\n",
    "\n",
    "        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)\n",
    "        if i_dataset == 0:\n",
    "            plt.title(name, size=18)\n",
    "\n",
    "        colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a',\n",
    "                                             '#f781bf', '#a65628', '#984ea3',\n",
    "                                             '#999999', '#e41a1c', '#dede00']),\n",
    "                                      int(max(y_pred) + 1))))\n",
    "        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n",
    "\n",
    "        plt.xlim(-2.5, 2.5)\n",
    "        plt.ylim(-2.5, 2.5)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        plot_num += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Case Elo7 - Motivos de Compra\n",
    "\n",
    "Os compradores do Elo7 são incentivados a indicar o motivo da compra de determinado produto no seu marketplace. Esses motivos nos ajudam a entender melhor o **momento** de compra do usuário. O dataset apresentado a seguir contém um subset desses motivos de compra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_reason = pd.read_csv(os.path.join(DATASET_FOLDER, 'purchase_reason_elo7_dataset.csv'), sep=';')\n",
    "\n",
    "df_reason.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existem muitos tipos possíveis de motivos de compra, mas será que nós podemos encontrar algum padrão neles? Me parece um problema clássico de **clustering**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos utilizar o Tf-Idf para criar o embedding dos motivos de compra e o K-Means para encontrar clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(max_df=0.9, max_features=5000, sublinear_tf=True, use_idf=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cria a matriz de embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como escolher o número de clusters? Vamos utilizar o gráfico de inércias. (Obs: outra possibilidade é avaliar o [\"silhouette score\"](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html#sphx-glr-auto-examples-cluster-plot-kmeans-silhouette-analysis-py))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicialize o K-Means com a quantidade de clusters que escolhemos a partir do gráfico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "kmeans = _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treine o modelo K-Means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encontre os clusters para cada motivo de compra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "labels = _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora visualizar os clusters criados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Crie um novo dataframe com os labels dos clusters\n",
    "df = pd.DataFrame({'reason': df_reason['reason'], 'labels': labels})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos verificar a distribuição de motivos em cada cluster. Quanto mais desbalanceado, pior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.groupby('labels').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos visualizar alguns exemplos de clusters gerados pelo K-Means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for idx in range(50):\n",
    "    idx_labels = df[df['labels']==idx]['reason'].unique()\n",
    "    print('- Cluster {}:'.format(idx + 1))\n",
    "    for i in np.random.choice(idx_labels, min(len(idx_labels), 10), replace=False):\n",
    "        print(' '*5, i)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qual é o resultado dos clusters gerados? Podemos avançar um pouco e verificar se existe alguma relação de hierarquia entre os motivos de compra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "\n",
    "# TODO\n",
    "Y = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Obtém aleatoriamente um dos motivos para representar o cluster\n",
    "titles = df.groupby('labels').apply(lambda x: np.random.choice(list(x['reason'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plota o dendrograma\n",
    "plt.figure(figsize=(16,10))\n",
    "dendrogram(Y,\n",
    "           labels=titles.values,\n",
    "           leaf_rotation=90,\n",
    "           leaf_font_size=14,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qual foi o resultado? O que você faria para melhorar o resultado obtido?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Case Elo7 - Subcategorias Automáticas\n",
    "\n",
    "Vamos para mais um case real do Elo7!\n",
    "\n",
    "Esse case é um dos trabalhos mais recentes do time de Data Science do Elo7. De fato, é um trabalho ainda em aberto e qualquer sugestão de melhorias é bem vinda! =)\n",
    "\n",
    "- O problema:\n",
    "O Elo7 possui uma árvore de categorias dividida em N1 e N2. O primeiro nível (N1) contém as categorias \"alto nível\" do site. São as categorias mais genéricas do marketplace- ou, pelo menos, é assim gostaríamos que fosse. As categorias N2, ou subcategorias, são as possíveis extensões dos nós das categorias N1. Podemos perceber que a árvore é extremamente limitada e isso é um problema grave não só para os compradores, que não conseguem navegar nas nossas categorias, mas também para os vendedores, que não conseguem categorizar bem seus produtos. A solução para esse problema seria uma árvore de categorias com maior \"granularidade\", ou seja, que consiga expandir além dos 2 níveis e ter mais subcategorias.\n",
    "\n",
    "- O que o time de Data Science tem a ver com essa história? \n",
    "\n",
    "Bom, gerar uma nova árvore de categoria pode ser uma tarefa bastante monótona e cansativa. Provavelmente deve haver algum jeito de encontrar bons agrupamentos de produtos que pudessem servir como uma nova subcategoria. Talvez algum método de clustering que utilize como features o conteúdo dos produtos pode gerar algum resultado interessante.\n",
    "\n",
    "- O experimento:\n",
    "\n",
    "O dataset a seguir possui um subconjunto de produtos que foram categorizados na categoria N1 \"Casamento\". Escolhemos esse conjunto de dados para iniciar nossos trabalhos, porque assim temos mais controle sobre nossos resultados. E, também, porque é uma das categorias mais importantes do marketplace.\n",
    "\n",
    "Para essa tarefa, vamos utilizar apenas o título e uma parte da descrição do produto (aprox. 140 caracteres) como features de entrada.\n",
    "\n",
    "Vamos analisar os dados!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_cat = pd.read_csv(os.path.join(DATASET_FOLDER, 'subcategory_elo7_dataset.csv'), sep=';')\n",
    "\n",
    "df_cat.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos criar uma coluna com as features que vamos incluir no nosso modelo de aprendizagem.\n",
    "Esse vetor de features será o título + descrição do produto. Para compensar a quantidade de palavras do título em relação a descrição, vamos repetir o título duas vezes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_cat['title_desc'] = (df_cat['title'] + ' ')*2 + df_cat['short_description']\n",
    "\n",
    "df_cat.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de agora é com você! Tente encontrar as subcategorias dos produtos da categoria \"casamento\". Lembre-se de que não queremos apenas aumentar o número de subcategorias do segundo nível (N2), mas também aumentar a profundidade da nossa árvore de categorias (N3, N4 ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Extra - DBSCAN\n",
    "\n",
    "\n",
    "#### Perfil usuários Elo7\n",
    "Vamos ver como ficaria o exemplo dos usuários do Elo7 utilizando DBSCAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "X = df_user_elo7.values\n",
    "\n",
    "# Dados normalizados\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "estimator = DBSCAN(eps=0.33, min_samples=10, metric='euclidean')\n",
    "\n",
    "estimator.fit(X_scaled)\n",
    "\n",
    "labels = estimator.labels_\n",
    "plt.scatter(x=X[:,0],\n",
    "            y=X[:,1],\n",
    "            c=labels, \n",
    "            edgecolor='k',\n",
    "            cmap='rainbow')\n",
    "plt.show()\n",
    "print(np.unique(labels))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
